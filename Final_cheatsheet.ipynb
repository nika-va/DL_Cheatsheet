{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrhs7qQGxqbsFA6ydxCPie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nika-va/DL_Cheatsheet/blob/main/Final_cheatsheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0. Data preprocessing"
      ],
      "metadata": {
        "id": "yXhTqWndD6fu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('sarcasm.json', 'r') as f:\n",
        "        datastore = json.load(f)\n",
        "\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type = 'post'\n",
        "    padding_type = 'post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    training_sentences = sentences[:training_size]\n",
        "    training_labels = labels[:training_size]\n",
        "    validation_sentences = sentences[training_size:]\n",
        "    validation_labels = labels[training_size:]\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n",
        "    validation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    training_padded = np.array(training_padded)\n",
        "    training_labels = np.array(training_labels)\n",
        "    validation_padded = np.array(validation_padded)\n",
        "    validation_labels = np.array(validation_labels)\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(24, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])"
      ],
      "metadata": {
        "id": "bcx6vJxdhpWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose cols when importing\n",
        "df_3 = pd.read_csv('Summary of Weather.csv', usecols=['STA', 'Precip','MaxTemp', 'MinTemp', 'YR', 'MO', 'DA'])"
      ],
      "metadata": {
        "id": "aql4HuPaEAnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change data type | replace values\n",
        "A = tf.cast(tf.range(-10, 10), tf.float32)\n",
        "\n",
        "precipitation_numeric = df_3['Precip'].replace('T', '0.01')\n",
        "precipitation_numeric = pd.to_numeric(precipitation_numeric)\n",
        "df_3['Precip'] = precipitation_numeric.astype(float)"
      ],
      "metadata": {
        "id": "kJODFsMsEAid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value counts/ info/ describe\n",
        "df_3[['STA', 'YR']].value_counts()\n",
        "df_3['STA'].nunique()"
      ],
      "metadata": {
        "id": "YEDyXYAmEAgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "\n",
        "from category_encoders.hashing import HashingEncoder\n",
        "from sklearn import preprocessing\n",
        "he = HashingEncoder(cols=['STA']).fit(df_3)\n",
        "df_3 = he.transform(df_3)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Fit and transform the categorical data\n",
        "columns_to_encode = ['Subscription Type', 'Country', 'Gender', 'Device', 'Age group']\n",
        "le = LabelEncoder()\n",
        "col_mapping = pd.DataFrame(columns=['Column', 'Original', 'Encoded'])\n",
        "for column in columns_to_encode:\n",
        "    netflix_df[column] = le.fit_transform(netflix_df[column])\n",
        "    for i in range(0,len(le.classes_)):\n",
        "        col_mapping.loc[len(col_mapping.index)] = [column, le.classes_[i], le.transform(le.classes_)[i]]"
      ],
      "metadata": {
        "id": "bgvDK3gVEAdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# scaling 0-1\n",
        "cols_to_scale = ['Precip',\t'MaxTemp',\t'MinTemp', 'YR',\t'MO',\t'DA']\n",
        "df_3[cols_to_scale] = preprocessing.MinMaxScaler().fit_transform(df_3[cols_to_scale]) # also StandardScaler"
      ],
      "metadata": {
        "id": "l3vMVoq1EjV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance\n",
        "# decision tree feature importances\n",
        "model = DecisionTreeRegressor()\n",
        "model.fit(X, y)\n",
        "importance = model.feature_importances_\n",
        "\n",
        "for i,v in enumerate(importance):\n",
        " print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "# xgboost feature importances\n",
        "model = XGBRegressor()\n",
        "model.fit(X, y)\n",
        "importance = model.feature_importances_\n",
        "\n",
        "for i,v in enumerate(importance):\n",
        " print('Feature: %0d, Score: %.5f' % (i,v))\n",
        "# plot feature importance\n",
        "plt.bar([x for x in range(len(importance))], importance)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KZovcEJcEjQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# outlier detection\n",
        "def find_outliers(columns, dataset = df):\n",
        "  outliers = []\n",
        "  for col in columns:\n",
        "    col_series = dataset[col].sort_values()\n",
        "\n",
        "    Q2 = col_series.median()\n",
        "    mid_data = round(len(col_series)/2)\n",
        "    Q1, Q3 = col_series[:mid_data].median(), col_series[mid_data:].median()\n",
        "    IQR = Q3 - Q1\n",
        "    data_max, data_min = Q3 + IQR *1.5, Q1 - IQR * 1.5\n",
        "\n",
        "    for i in col_series:\n",
        "      if i > data_max or i < data_min:\n",
        "        outliers.append([col, i])\n",
        "\n",
        "  return pd.pivot_table(pd.DataFrame(outliers, columns=['columns', 'outlier']), index=['columns', 'outlier'])\n",
        "\n",
        "find_outliers(['Sales', 'Shipping Delay'])"
      ],
      "metadata": {
        "id": "uUQQwUGPEAAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fill na\n",
        "df['age'].fillna(round(df['age'].mean(), 2), inplace=True)\n",
        "df['Age'] = df['Age'].fillna(round(df['Age'].mean(), 2))"
      ],
      "metadata": {
        "id": "MpTm7TGEE5hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datetime\n",
        "df = pd.read_excel('/content/Diplom iÅŸi data x2.xlsx', converters= {'Date': pd.to_datetime})\n",
        "df.head()\n",
        "\n",
        "df['year'] = df['Date'].dt.year\n",
        "df['month'] = df['Date'].dt.month\n",
        "df['day'] = df['Date'].dt.day"
      ],
      "metadata": {
        "id": "GglZhAGdE5eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download mnist\n",
        "# Get current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Append data/mnist.npz to the previous path to get the full path\n",
        "data_path = os.path.join(current_dir, \"data/mnist.npz\")\n",
        "\n",
        "# Get only training set\n",
        "(training_images, training_labels), _ = tf.keras.datasets.mnist.load_data(path=data_path)"
      ],
      "metadata": {
        "id": "SvJSonXSE5XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# end before 99% accuracy\n",
        "class myCallback(keras.callbacks.Callback):\n",
        "\n",
        "     def on_epoch_end(self, epoch, logs=None):\n",
        "        if logs.get('accuracy') is not None and logs.get('accuracy') > 0.995:\n",
        "            print(\"\\nReached 99.5% accuracy so cancelling training!\")\n",
        "\n",
        "                # Stop training once the above condition is met\n",
        "            self.model.stop_training = True\n"
      ],
      "metadata": {
        "id": "58eRBoTvPRmD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IPI1h6J8lUP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Regression"
      ],
      "metadata": {
        "id": "zvffEX98FztT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "wO9068DZF5qX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Pqo6sq7hj0h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912236fa-3fe2-4706-bd5f-eb8bcf17ce55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Create features (using tensors)\n",
        "X = np.arange(-100, 100, 4)\n",
        "\n",
        "# Create labels (using tensors)\n",
        "y = np.arange(-90, 110, 4)\n",
        "# Split data into train and test sets\n",
        "X_train = X[:40] # first 40 examples (80% of data)\n",
        "y_train = y[:40]\n",
        "\n",
        "X_test = X[40:] # last 10 examples (20% of data)\n",
        "y_test = y[40:]\n",
        "\n",
        "len(X_train), len(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(1,)),\n",
        "    tf.keras.layers.Dense(8, activation='relu'),\n",
        "    # tf.keras.layers.Dense(8, activation='relu'),\n",
        "    # tf.keras.layers.Dense(8, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.mae,\n",
        "              optimizer='adam',\n",
        "              metrics=['mae'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=10,callbacks=[tf.keras.callbacks.EarlyStopping(patience=10),\n",
        "                                                                #lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(epoch/20))\n",
        "                                                                 tf.keras.callbacks.ModelCheckpoint('saved_model.keras', save_best_only=True)],\n",
        "          validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "\n",
        "y_preds = model.predict(X_test).squeeze()\n",
        "y_preds\n",
        "model.evaluate(X_test, y_test)\n",
        "mean_absolute_error(y_test, y_preds)\n",
        "\n",
        "model.save('saved_model.h5')\n",
        "saved_model = tf.keras.models.load_model('saved_model.h5')\n",
        "print(saved_model.evaluate(X_test, y_test))\n",
        "\n",
        "saved_model = tf.keras.models.load_model('saved_model.keras')\n",
        "saved_model.evaluate(X_test, y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDat3brSF22j",
        "outputId": "14ef5c7c-82ea-4ce6-8a50-3506c29572d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 24 calls to <function Model.make_test_function.<locals>.test_function at 0x78124d93d090> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x78124d93d480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 61ms/step\n",
            "1/1 [==============================] - 0s 114ms/step - loss: 88.7591 - mae: 88.7591\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 124ms/step - loss: 88.7591 - mae: 88.7591\n",
            "[88.75910186767578, 88.75910186767578]\n",
            "1/1 [==============================] - 0s 187ms/step - loss: 88.7591 - mae: 88.7591\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[88.75910186767578, 88.75910186767578]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiFSSPhCL7C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Classification"
      ],
      "metadata": {
        "id": "5RVz9-lmGkZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(shape=(11,)),\n",
        "    # tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(16, activation='relu'),\n",
        "    # tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy() ,optimizer=tf.keras.optimizers.Adam(), metrics=['accuracy'])\n",
        "model.fit(X_train, y_train,\n",
        "          epochs=15,\n",
        "          batch_size=128,\n",
        "          validation_data=(X_test, y_test),\n",
        "          callbacks=[\n",
        "                    tf.keras.callbacks.EarlyStopping(patience=3),\n",
        "                    tf.keras.callbacks.ModelCheckpoint('checkpoints/model_made.keras', save_best_only=True),\n",
        "                    tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', patience=5)])"
      ],
      "metadata": {
        "id": "Sm01oqKNGmth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. CNN"
      ],
      "metadata": {
        "id": "g1prvy7V5qnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Download zip file of pizza_steak images\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/pizza_steak.zip\n",
        "\n",
        "# Unzip the downloaded file\n",
        "zip_ref = zipfile.ZipFile(\"pizza_steak.zip\", \"r\")\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "gpH9mryx5tkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Walk through pizza_steak directory and list number of files\n",
        "for dirpath, dirnames, filenames in os.walk(\"pizza_steak\"):\n",
        "  print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
      ],
      "metadata": {
        "id": "qoOzGG365xt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the class names (programmatically, this is much more helpful with a longer list of classes)\n",
        "import pathlib\n",
        "import numpy as np\n",
        "data_dir = pathlib.Path(\"pizza_steak/train/\") # turn our training path into a Python path\n",
        "class_names = np.array(sorted([item.name for item in data_dir.glob('*')])) # created a list of class_names from the subdirectories\n",
        "print(class_names)"
      ],
      "metadata": {
        "id": "VuwZY6LC596b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "# !pip install Pillow\n",
        "# from PIL import Image\n",
        "# import PIL\n",
        "# import sys\n",
        "# sys.modules['Image'] = Image\n",
        "# print(Image.__file__)\n",
        "\n",
        "# Set the seed\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "                                  # rotation_range=20, # rotate the image slightly between 0 and 20 degrees (note: this is an int not a float)\n",
        "                                  # shear_range=0.2, # shear the image\n",
        "                                  # zoom_range=0.2, # zoom into the image\n",
        "                                  # width_shift_range=0.2, # shift the image width ways\n",
        "                                  # height_shift_range=0.2, # shift the image height ways\n",
        "                                  # horizontal_flip=True) # flip the image on the horizontal axis\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Setup the train and test directories\n",
        "train_dir = \"pizza_steak/train/\"\n",
        "test_dir = \"pizza_steak/test/\"\n",
        "\n",
        "# Import data from directories and turn it into batches\n",
        "train_data = train_datagen.flow_from_directory(train_dir,\n",
        "                                               batch_size=32, # number of images to process at a time\n",
        "                                               target_size=(224, 224), # convert all images to be 224 x 224\n",
        "                                               class_mode=\"binary\", # type of problem we're working on\n",
        "                                               seed=42)\n",
        "\n",
        "valid_data = valid_datagen.flow_from_directory(test_dir,\n",
        "                                               batch_size=32,\n",
        "                                               target_size=(224, 224),\n",
        "                                               class_mode=\"binary\",\n",
        "                                               seed=42)\n",
        "\n",
        "# train_data.target_size, train_data.class_indices"
      ],
      "metadata": {
        "id": "mE1qmeZS6bEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = keras.Sequential([\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.2),\n",
        "  layers.RandomZoom(0.2),\n",
        "  layers.RandomHeight(0.2),\n",
        "  layers.RandomWidth(0.2),\n",
        "  # preprocessing.Rescaling(1./255) # keep for ResNet50V2, remove for EfficientNetV2B0\n",
        "], name =\"data_augmentation\")"
      ],
      "metadata": {
        "id": "ebTMLOOogQ-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SEQUENTIAL MODEL\n",
        "model_1 = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(filters=10,\n",
        "                         kernel_size=3, # can also be (3, 3)\n",
        "                         activation=\"relu\",\n",
        "                         input_shape=(224, 224, 3)), # first layer specifies input shape (height, width, colour channels)\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.MaxPool2D(pool_size=2, # pool_size can also be (2, 2)\n",
        "                            padding=\"valid\"), # padding can also be 'same'\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"),\n",
        "  tf.keras.layers.Conv2D(10, 3, activation=\"relu\"), # activation='relu' == tf.keras.layers.Activations(tf.nn.relu)\n",
        "  tf.keras.layers.MaxPool2D(2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(1, activation=\"sigmoid\") # binary activation output\n",
        "])\n",
        "# Compile the model\n",
        "model_1.compile(loss=\"binary_crossentropy\", #'categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "# Fit the model\n",
        "history_1 = model_1.fit(train_data,\n",
        "                        epochs=5,\n",
        "                        steps_per_epoch=len(train_data),\n",
        "                        validation_data=valid_data,\n",
        "                        validation_steps=len(valid_data))\n",
        "\n",
        "# FUNCTIONAL MODEL\n",
        "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
        "x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(inputs)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
        "x = tf.keras.layers.MaxPooling2D((2, 2))(x)\n",
        "x = tf.keras.layers.Flatten()(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(10, activation='softmax')(x)\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "RyrRq9E06-my",
        "outputId": "a4f77562-913b-4eb3-d618-3cb807d70b01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tf' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-3e19783bf4cd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# SEQUENTIAL MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model_1 = tf.keras.models.Sequential([\n\u001b[0m\u001b[1;32m      3\u001b[0m   tf.keras.layers.Conv2D(filters=10, \n\u001b[1;32m      4\u001b[0m                          \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# can also be (3, 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                          \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to import an image and resize it to be able to be used with our model\n",
        "def load_and_prep_image(filename, img_shape=224):\n",
        "  \"\"\"\n",
        "  Reads an image from filename, turns it into a tensor\n",
        "  and reshapes it to (img_shape, img_shape, colour_channel).\n",
        "  \"\"\"\n",
        "  # Read in target file (an image)\n",
        "  img = tf.io.read_file(filename)\n",
        "\n",
        "  # Decode the read file into a tensor & ensure 3 colour channels\n",
        "  # (our model is trained on images with 3 colour channels and sometimes images have 4 colour channels)\n",
        "  img = tf.image.decode_image(img, channels=3)\n",
        "\n",
        "  # Resize the image (to the same size our model was trained on)\n",
        "  img = tf.image.resize(img, size = [img_shape, img_shape])\n",
        "\n",
        "  # Rescale the image (get all values between 0 and 1)\n",
        "  img = img/255.\n",
        "  return img"
      ],
      "metadata": {
        "id": "9HHIYDof_B19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oU-XsnLwHF0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Feature Extraction & Fine tuning"
      ],
      "metadata": {
        "id": "mGvp8BSlNF7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/search?q=image+classification+in%3Amodels+modelFramework%3A%22TensorFlow+2%22   # image classification models"
      ],
      "metadata": {
        "id": "GjaeW31UNPC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORT DATASET\n",
        " (x_train, y_train), (x_test, y_test) = tfds.as_numpy(tfds.load(\n",
        "    'mnist_corrupted',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    batch_size=-1,\n",
        "    as_supervised=True))\n",
        "\n",
        "x_train_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_train, dtype=tf.float32))\n",
        "x_test_rgb = tf.image.grayscale_to_rgb(tf.convert_to_tensor(x_test, dtype=tf.float32))\n",
        "\n",
        "x_train_rgb = x_train_rgb / 255.0\n",
        "x_test_rgb = x_test_rgb / 255.0\n",
        "\n",
        "# IMPORT\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# IMPORT\n",
        "mnist = tf.keras.datasets.mnist\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "    # Normalize the images to [0, 1] range\n",
        "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "    # Expand the dimensions to include the channel dimension (28, 28, 1)\n",
        "    x_train = x_train[..., tf.newaxis]\n",
        "    x_test = x_test[..., tf.newaxis]"
      ],
      "metadata": {
        "id": "INf-7RUxZrij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(\"https://www.kaggle.com/models/tensorflow/resnet-50/TensorFlow2/feature-vector/1\",\n",
        "                   trainable=False),  # Can be True, see below.\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "obE0fNflZXg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FEATURE ENGINEERING\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create inputs into the base model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "\n",
        "# x = data_augmentation(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNetV2 inputs don't have to be normalized)\n",
        "x = base_model(inputs)\n",
        "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "model_0 = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "Qr1mz1RSdwRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FINE TUNING\n",
        "\n",
        "# there are tf.keras.applications models & kaggle tensorflow models\n",
        "# image input should be none none 3\n",
        "# model is created if given input_shape, if not, then in the fit\n",
        "# Access the base_model layers of model_2\n",
        "model_2_base_model = model_2.layers[2]\n",
        "model_2_base_model.trainable = True\n",
        "# for layer_number, layer in enumerate(model_2_base_model.layers):\n",
        "#   print(layer_number, layer.name, layer.trainable)\n",
        "for layer in model_2_base_model.layers[:-10]:\n",
        "  layer.trainable = False\n",
        "# Recompile the whole model (always recompile after any adjustments to a model)\n",
        "model_2.compile(loss=\"categorical_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), # lr is 10x lower than before for fine-tuning\n",
        "                metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "fine_tune_epochs = initial_epochs + 5\n",
        "# Refit the model (same as model_2 except with more trainable layers)\n",
        "history_fine_10_percent_data_aug = model_2.fit(train_data_10_percent,\n",
        "                                               epochs=fine_tune_epochs,\n",
        "                                               validation_data=test_data,\n",
        "                                               initial_epoch=history_10_percent_data_aug.epoch[-1], # start from previous last epoch\n",
        "                                               validation_steps=int(0.25 * len(test_data)),\n",
        "                                               callbacks=[create_tensorboard_callback(\"transfer_learning\", \"10_percent_fine_tune_last_10\")]) # name experiment appropriately\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "base_model = tf.keras.applications.efficientnet_v2.EfficientNetV2B0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# 3. Create inputs into the base model\n",
        "inputs = tf.keras.layers.Input(shape=(224, 224, 3))\n",
        "\n",
        "# 4. If using ResNet50V2, add this to speed up convergence, remove for EfficientNetV2\n",
        "# x = tf.keras.layers.experimental.preprocessing.Rescaling(1./255)(inputs)\n",
        "\n",
        "# x = data_augmentation(inputs)\n",
        "\n",
        "# 5. Pass the inputs to the base_model (note: using tf.keras.applications, EfficientNetV2 inputs don't have to be normalized)\n",
        "x = base_model(inputs) # , trainable = False\n",
        "# 6. Average pool the outputs of the base model (aggregate all the most important information, reduce number of computations)\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "# 7. Create the output activation layer\n",
        "outputs = tf.keras.layers.Dense(10, activation=\"softmax\", name=\"output_layer\")(x)\n",
        "model_0 = tf.keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "3q4aNh6Seykb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "1a4f75c4-17bc-423e-ef3d-a75e08f9f25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ac49b687ce8e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model is created if given input_shape, if not, then in the fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Access the base_model layers of model_2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_2_base_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel_2_base_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# for layer_number, layer in enumerate(model_2_base_model.layers):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_HCK1MUdwJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. NLP"
      ],
      "metadata": {
        "id": "7mCUs0eC1nyY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
        "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
        "                                    split=\"whitespace\", # how to split tokens\n",
        "                                    ngrams=None, # create groups of n-words?\n",
        "                                    output_mode=\"int\", # how to map tokens to numbers\n",
        "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
        "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None\n",
        "\n",
        "text_vectorizer.adapt(train_sentences)\n",
        "\n",
        "vocab_size = len(text_vectorizer.word_index) + 1\n"
      ],
      "metadata": {
        "id": "yhZvTB5f1pEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after text vectorization\n",
        "embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n",
        "                             output_dim=128, # set size of embedding vector\n",
        "                             embeddings_initializer=\"uniform\", # default, intialize randomly\n",
        "                             input_length=max_length, # how long is each input\n",
        "                             name=\"embedding_1\")\n",
        "\n",
        "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
        "sample_embed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "QwyVCA5545pe",
        "outputId": "16c3139e-3f0d-4398-9f8c-5862c47bdbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'layers' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-8ba4973d8bb1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m embedding = layers.Embedding(input_dim=max_vocab_length, # set input shape\n\u001b[0m\u001b[1;32m      2\u001b[0m                              \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# set size of embedding vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                              \u001b[0membeddings_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# default, intialize randomly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                              \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# how long is each input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              name=\"embedding_1\")\n",
            "\u001b[0;31mNameError\u001b[0m: name 'layers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NAIVE BAYES\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create tokenization and modelling pipeline\n",
        "model_0 = Pipeline([\n",
        "                    (\"tfidf\", TfidfVectorizer()), # convert words to numbers using tfidf\n",
        "                    (\"clf\", MultinomialNB()) # model the text\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data\n",
        "model_0.fit(train_sentences, train_labels)"
      ],
      "metadata": {
        "id": "0TzZK6nA5EhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SIMPLE DENSE MODEL\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
        "x = text_vectorizer(inputs) # turn the input text into numbers\n",
        "x = embedding(x) # create an embedding of the numerized numbers\n",
        "x = layers.GlobalAveragePooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
        "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\")"
      ],
      "metadata": {
        "id": "dpjWb84q78wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN models\n",
        "\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "print(inputs.shape)\n",
        "x = text_vectorizer(inputs)\n",
        "print(x.shape)\n",
        "x = model_2_embedding(x)\n",
        "print(x.shape)\n",
        "\n",
        "# x = layers.LSTM(64, return_sequences=True)(x) # return vector for each word in the Tweet (you can stack RNN cells as long as return_sequences=True)\n",
        "x = layers.LSTM(64)(x) # return vector for whole sequence\n",
        "print(x.shape)\n",
        "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer on top of output of LSTM cell\n",
        "\n",
        "# OR\n",
        "\n",
        "# x = layers.GRU(64, return_sequences=True) # stacking recurrent cells requires return_sequences=True\n",
        "x = layers.GRU(64)(x)\n",
        "# x = layers.Dense(64, activation=\"relu\")(x) # optional dense layer after GRU cell\n",
        "\n",
        "# OR\n",
        "\n",
        "# x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x) # stacking RNN layers requires return_sequences=True\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x) # bidirectional goes both ways so has double the parameters of a regular LSTM layer\n",
        "\n",
        "# OR\n",
        "\n",
        "conv_1d = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\") # convolve over target sequence 5 words at a time\n",
        "conv_1d_output = conv_1d(embedding_test) # pass embedding through 1D convolutional layer\n",
        "max_pool = layers.GlobalMaxPool1D()\n",
        "max_pool_output = max_pool(conv_1d_output) # get the most important features\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model_2 = tf.keras.Model(inputs, outputs, name=\"model_2_LSTM\")"
      ],
      "metadata": {
        "id": "pfLw6KMd0_0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transfer learning\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "# We can use this encoding layer in place of our text_vectorizer and embedding layer\n",
        "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                                        input_shape=[], # shape of inputs coming to our model\n",
        "                                        dtype=tf.string, # data type of inputs coming to the USE layer\n",
        "                                        trainable=False, # keep the pretrained weights (we'll create a feature extractor)\n",
        "                                        name=\"USE\")\n",
        "model_6 = tf.keras.Sequential([\n",
        "  sentence_encoder_layer, # take in sentences and then encode them into an embedding\n",
        "  layers.Dense(64, activation=\"relu\"),\n",
        "  layers.Dense(1, activation=\"sigmoid\")\n",
        "], name=\"model_6_USE\")\n",
        "\n",
        "# Transfer Learning\n",
        "\n",
        "# text_vectorizer = TextVectorization(max_tokens=35000,\n",
        "#                                     standardize=\"lower_and_strip_punctuation\",\n",
        "#                                     split=\"whitespace\",  # how to split tokens\n",
        "#                                     ngrams=None,  # create groups of n-words?\n",
        "#                                     output_mode=\"int\",  # how to map tokens to numbers\n",
        "#                                     output_sequence_length=None,)  # how long should the output sequence of tokens be?\n",
        "#                                     # pad_to_max_tokens=True)  # Not valid if using max_tokens=None\n",
        "# text_vectorizer.adapt(train_df['combined_text'])\n",
        "class UniversalSentenceEncoderLayer(tf.keras.layers.Layer):\n",
        "    def call(self, inputs):\n",
        "        # Reshape the input to remove the extra dimension\n",
        "        sentence_encoder_layer = tf_keras.Sequential(hub.KerasLayer(\n",
        "            \"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\",\n",
        "            input_shape=[],  # shape of inputs coming to our model\n",
        "            dtype=tf.string,  # data type of inputs coming to the USE layer\n",
        "            trainable=False,  # keep the pretrained weights (we'll create a feature extractor)\n",
        "            name=\"USE\"))\n",
        "\n",
        "        return sentence_encoder_layer(inputs)\n",
        "\n",
        "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "# x = text_vectorizer(inputs)\n",
        "# x = tf.keras.layers.Embedding(35000, 128)(x)\n",
        "x = UniversalSentenceEncoderLayer()(inputs)\n",
        "# x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(x)\n",
        "x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
        "outputs = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_df['combined_text'].to_numpy(), train_df['answer_encoded'].to_numpy(), epochs=10, batch_size=32, validation_data=(test_df['combined_text'].to_numpy(), test_df['answer_encoded'].to_numpy()))"
      ],
      "metadata": {
        "id": "lkL8JozU1DtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone model_6 but reset weights\n",
        "model_7 = tf.keras.models.clone_model(model_6)\n",
        "\n",
        "# Compile model\n",
        "model_7.compile(loss=\"binary_crossentropy\",\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "qFNHmriQ7fNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ENSEMBLE\n",
        "baseline_pred_probs = np.max(model_0.predict_proba(val_sentences), axis=1) # get the prediction probabilities from baseline model\n",
        "combined_pred_probs = baseline_pred_probs + tf.squeeze(model_2_pred_probs, axis=1) + tf.squeeze(model_6_pred_probs)\n",
        "combined_preds = tf.round(combined_pred_probs/3) # average and round the prediction probabilities to get prediction classes\n",
        "combined_preds[:20]\n"
      ],
      "metadata": {
        "id": "fFmcS_AtCKHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7SA67yVeGnQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Time Series"
      ],
      "metadata": {
        "id": "98Y7q43vRJJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INTERESTING TO KNOW\n",
        "# 1. Turn train and test arrays into tensor Datasets\n",
        "train_features_dataset = tf.data.Dataset.from_tensor_slices(X_train)\n",
        "train_labels_dataset = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "\n",
        "test_features_dataset = tf.data.Dataset.from_tensor_slices(X_test)\n",
        "test_labels_dataset = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "# 2. Combine features & labels\n",
        "train_dataset = tf.data.Dataset.zip((train_features_dataset, train_labels_dataset))\n",
        "test_dataset = tf.data.Dataset.zip((test_features_dataset, test_labels_dataset))\n",
        "\n",
        "# 3. Batch and prefetch for optimal performance\n",
        "BATCH_SIZE = 1024 # taken from Appendix D in N-BEATS paper\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "yEoBZvvZacM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/household_power.zip'\n",
        "    urllib.request.urlretrieve(url, 'household_power.zip')\n",
        "    with zipfile.ZipFile('household_power.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()"
      ],
      "metadata": {
        "id": "J4wRtJryRL6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "naive_forecast = y_test[:-1] # NaÃ¯ve forecast equals every value excluding the last value\n",
        "naive_forecast[:10], naive_forecast[-10:]"
      ],
      "metadata": {
        "id": "32lisht4S8yC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_windows(x, window_size=7, horizon=1):\n",
        "  \"\"\"\n",
        "  Turns a 1D array into a 2D array of sequential windows of window_size.\n",
        "  \"\"\"\n",
        "  # 1. Create a window of specific window_size (add the horizon on the end for later labelling)\n",
        "  window_step = np.expand_dims(np.arange(window_size+horizon), axis=0)\n",
        "  # print(f\"Window step:\\n {window_step}\")\n",
        "\n",
        "  # 2. Create a 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
        "  window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)), axis=0).T # create 2D array of windows of size window_size\n",
        "  # print(f\"Window indexes:\\n {window_indexes[:3], window_indexes[-3:], window_indexes.shape}\")\n",
        "\n",
        "  # 3. Index on the target array (time series) with 2D array of multiple window steps\n",
        "  windowed_array = x[window_indexes]\n",
        "\n",
        "  # 4. Get the labelled windows\n",
        "  windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
        "\n",
        "  return windows, labels\n",
        "\n",
        "# ORR\n",
        "\n",
        "# Add windowed columns\n",
        "for i in range(WINDOW_SIZE): # Shift values for each step in WINDOW_SIZE\n",
        "  bitcoin_prices_windowed[f\"Price+{i+1}\"] = bitcoin_prices_windowed[\"Price\"].shift(periods=i+1)\n",
        "bitcoin_prices_windowed.head(10)\n",
        "X = bitcoin_prices_windowed.dropna().drop(\"Price\", axis=1).astype(np.float32)\n",
        "y = bitcoin_prices_windowed.dropna()[\"Price\"].astype(np.float32)\n",
        "X.head()\n",
        "\n",
        "# ORR\n",
        "\n",
        "def windowed_dataset(series, batch_size, n_past=24, n_future=24, shift=1):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(size=n_past + n_future, shift=shift, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(n_past + n_future))\n",
        "    ds = ds.map(lambda w: (w[:n_past], w[n_past:]))\n",
        "    return ds.batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "pliMGpe1S9QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before we pass our data to the Conv1D layer, we have to reshape it in order to make sure it works\n",
        "x = tf.constant(train_windows[0])  # (7,)\n",
        "expand_dims_layer = layers.Lambda(lambda x: tf.expand_dims(x, axis=1)) #(7,1)"
      ],
      "metadata": {
        "id": "WpAdQM0BT3KZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONV1D\n",
        "\n",
        "model_4 = tf.keras.Sequential([\n",
        "  # Create Lambda layer to reshape inputs, without this layer, the model will error\n",
        "  layers.Lambda(lambda x: tf.expand_dims(x, axis=1)), # resize the inputs to adjust for window size / Conv1D 3D input requirements\n",
        "  layers.Conv1D(filters=128, kernel_size=5, padding=\"causal\", activation=\"relu\"),\n",
        "  layers.Dense(HORIZON)\n",
        "], name=\"model_4_conv1D\")\n",
        "\n",
        "# Compile model\n",
        "model_4.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Fit model\n",
        "model_4.fit(train_windows,\n",
        "            train_labels,\n",
        "            batch_size=128,\n",
        "            epochs=100,\n",
        "            verbose=0,\n",
        "            validation_data=(test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name=model_4.name)])"
      ],
      "metadata": {
        "id": "f59-rFV9YF2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM\n",
        "\n",
        "# Let's build an LSTM model with the Functional API\n",
        "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
        "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs) # expand input dimension to be compatible with LSTM\n",
        "# print(x.shape)\n",
        "# x = layers.LSTM(128, activation=\"relu\", return_sequences=True)(x) # this layer will error if the inputs are not the right shape\n",
        "x = layers.LSTM(128, activation=\"relu\")(x) # using the tanh loss function results in a massive error\n",
        "# print(x.shape)\n",
        "# Add another optional dense layer (you could add more of these to see if they improve model performance)\n",
        "# x = layers.Dense(32, activation=\"relu\")(x)\n",
        "output = layers.Dense(HORIZON)(x)\n",
        "model_5 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_5_lstm\")\n",
        "\n",
        "# Compile model\n",
        "model_5.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554\n",
        "model_5.fit(train_windows,\n",
        "            train_labels,\n",
        "            epochs=100,\n",
        "            verbose=0,\n",
        "            batch_size=128,\n",
        "            validation_data=(test_windows, test_labels),\n",
        "            callbacks=[create_model_checkpoint(model_name=model_5.name)])"
      ],
      "metadata": {
        "id": "54mbKg2KYYGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N BEATS\n",
        "\n",
        "# Create NBeatsBlock custom layer\n",
        "class NBeatsBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self, # the constructor takes all the hyperparameters for the layer\n",
        "               input_size: int,\n",
        "               theta_size: int,\n",
        "               horizon: int,\n",
        "               n_neurons: int,\n",
        "               n_layers: int,\n",
        "               **kwargs): # the **kwargs argument takes care of all of the arguments for the parent class (input_shape, trainable, name)\n",
        "    super().__init__(**kwargs)\n",
        "    self.input_size = input_size\n",
        "    self.theta_size = theta_size\n",
        "    self.horizon = horizon\n",
        "    self.n_neurons = n_neurons\n",
        "    self.n_layers = n_layers\n",
        "\n",
        "    # Block contains stack of 4 fully connected layers each has ReLU activation\n",
        "    self.hidden = [tf.keras.layers.Dense(n_neurons, activation=\"relu\") for _ in range(n_layers)]\n",
        "    # Output of block is a theta layer with linear activation\n",
        "    self.theta_layer = tf.keras.layers.Dense(theta_size, activation=\"linear\", name=\"theta\")\n",
        "\n",
        "  def call(self, inputs): # the call method is what runs when the layer is called\n",
        "    x = inputs\n",
        "    for layer in self.hidden: # pass inputs through each hidden layer\n",
        "      x = layer(x)\n",
        "    theta = self.theta_layer(x)\n",
        "    # Output the backcast and forecast from theta\n",
        "    backcast, forecast = theta[:, :self.input_size], theta[:, -self.horizon:]\n",
        "    return backcast, forecast\n",
        "\n",
        "nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n",
        "                                 theta_size=THETA_SIZE,\n",
        "                                 horizon=HORIZON,\n",
        "                                 n_neurons=N_NEURONS,\n",
        "                                 n_layers=N_LAYERS,\n",
        "                                 name=\"InitialBlock\")"
      ],
      "metadata": {
        "id": "QVGzU8hnZtNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Values from N-BEATS paper Figure 1 and Table 18/Appendix D\n",
        "N_EPOCHS = 5000 # called \"Iterations\" in Table 18\n",
        "N_NEURONS = 512 # called \"Width\" in Table 18\n",
        "N_LAYERS = 4\n",
        "N_STACKS = 30\n",
        "\n",
        "INPUT_SIZE = WINDOW_SIZE * HORIZON # called \"Lookback\" in Table 18  # 7 and 1\n",
        "THETA_SIZE = INPUT_SIZE + HORIZON\n",
        "\n",
        "INPUT_SIZE, THETA_SIZE"
      ],
      "metadata": {
        "id": "25SVGT-Ka2ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup N-BEATS Block layer\n",
        "nbeats_block_layer = NBeatsBlock(input_size=INPUT_SIZE,\n",
        "                                 theta_size=THETA_SIZE,\n",
        "                                 horizon=HORIZON,\n",
        "                                 n_neurons=N_NEURONS,\n",
        "                                 n_layers=N_LAYERS,\n",
        "                                 name=\"InitialBlock\")\n",
        "\n",
        "# 2. Create input to stacks\n",
        "stack_input = layers.Input(shape=(INPUT_SIZE), name=\"stack_input\")\n",
        "\n",
        "# 3. Create initial backcast and forecast input (backwards predictions are referred to as residuals in the paper)\n",
        "backcast, forecast = nbeats_block_layer(stack_input)\n",
        "# Add in subtraction residual link, thank you to: https://github.com/mrdbourke/tensorflow-deep-learning/discussions/174\n",
        "residuals = layers.subtract([stack_input, backcast], name=f\"subtract_00\")\n",
        "\n",
        "# 4. Create stacks of blocks\n",
        "for i, _ in enumerate(range(N_STACKS-1)): # first stack is already creted in (3)\n",
        "\n",
        "  # 5. Use the NBeatsBlock to calculate the backcast as well as block forecast\n",
        "  backcast, block_forecast = NBeatsBlock(\n",
        "      input_size=INPUT_SIZE,\n",
        "      theta_size=THETA_SIZE,\n",
        "      horizon=HORIZON,\n",
        "      n_neurons=N_NEURONS,\n",
        "      n_layers=N_LAYERS,\n",
        "      name=f\"NBeatsBlock_{i}\"\n",
        "  )(residuals) # pass it in residuals (the backcast)\n",
        "\n",
        "  # 6. Create the double residual stacking\n",
        "  residuals = layers.subtract([residuals, backcast], name=f\"subtract_{i}\")\n",
        "  forecast = layers.add([forecast, block_forecast], name=f\"add_{i}\")\n",
        "\n",
        "# 7. Put the stack model together\n",
        "model_7 = tf.keras.Model(inputs=stack_input,\n",
        "                         outputs=forecast,\n",
        "                         name=\"model_7_N-BEATS\")\n",
        "\n",
        "# 8. Compile with MAE loss and Adam optimizer\n",
        "model_7.compile(loss=\"mae\",\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "# 9. Fit the model with EarlyStopping and ReduceLROnPlateau callbacks\n",
        "model_7.fit(train_dataset,\n",
        "            epochs=N_EPOCHS,\n",
        "            validation_data=test_dataset,\n",
        "            verbose=0, # prevent large amounts of training outputs\n",
        "            # callbacks=[create_model_checkpoint(model_name=stack_model.name)] # saving model every epoch consumes far too much time\n",
        "            callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=200, restore_best_weights=True),\n",
        "                      tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", patience=100, verbose=1)])"
      ],
      "metadata": {
        "id": "3X9LEAPNbun_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------------"
      ],
      "metadata": {
        "id": "4DLLNlxkcwVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ensemble_models(horizon=HORIZON,\n",
        "                        train_data=train_dataset,\n",
        "                        test_data=test_dataset,\n",
        "                        num_iter=10,\n",
        "                        num_epochs=100,\n",
        "                        loss_fns=[\"mae\", \"mse\", \"mape\"]):\n",
        "  \"\"\"\n",
        "  Returns a list of num_iter models each trained on MAE, MSE and MAPE loss.\n",
        "\n",
        "  For example, if num_iter=10, a list of 30 trained models will be returned:\n",
        "  10 * len([\"mae\", \"mse\", \"mape\"]).\n",
        "  \"\"\"\n",
        "  # Make empty list for trained ensemble models\n",
        "  ensemble_models = []\n",
        "\n",
        "  # Create num_iter number of models per loss function\n",
        "  for i in range(num_iter):\n",
        "    # Build and fit a new model with a different loss function\n",
        "    for loss_function in loss_fns:\n",
        "      print(f\"Optimizing model by reducing: {loss_function} for {num_epochs} epochs, model number: {i}\")\n",
        "\n",
        "      # Construct a simple model (similar to model_1)\n",
        "      model = tf.keras.Sequential([\n",
        "        # Initialize layers with normal (Gaussian) distribution so we can use the models for prediction\n",
        "        # interval estimation later: https://www.tensorflow.org/api_docs/python/tf/keras/initializers/HeNormal\n",
        "        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
        "        layers.Dense(128, kernel_initializer=\"he_normal\", activation=\"relu\"),\n",
        "        layers.Dense(HORIZON)\n",
        "      ])\n",
        "\n",
        "      # Compile simple model with current loss function\n",
        "      model.compile(loss=loss_function,\n",
        "                    optimizer=tf.keras.optimizers.Adam(),\n",
        "                    metrics=[\"mae\", \"mse\"])\n",
        "\n",
        "      # Fit model\n",
        "      model.fit(train_data,\n",
        "                epochs=num_epochs,\n",
        "                verbose=0,\n",
        "                validation_data=test_data,\n",
        "                # Add callbacks to prevent training from going/stalling for too long\n",
        "                callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                                            patience=200,\n",
        "                                                            restore_best_weights=True),\n",
        "                           tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                                patience=100,\n",
        "                                                                verbose=1)])\n",
        "\n",
        "      # Append fitted model to list of ensemble models\n",
        "      ensemble_models.append(model)\n",
        "\n",
        "  return ensemble_models # return list of trained models"
      ],
      "metadata": {
        "id": "LXbU6p3zcunX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of trained ensemble models\n",
        "ensemble_models = get_ensemble_models(num_iter=5,\n",
        "                                      num_epochs=1000)\n",
        "# Create a function which uses a list of trained models to make and return a list of predictions\n",
        "def make_ensemble_preds(ensemble_models, data):\n",
        "  ensemble_preds = []\n",
        "  for model in ensemble_models:\n",
        "    preds = model.predict(data) # make predictions with current ensemble model\n",
        "    ensemble_preds.append(preds)\n",
        "  return tf.constant(tf.squeeze(ensemble_preds))\n",
        "\n",
        "ensemble_preds = make_ensemble_preds(ensemble_models=ensemble_models,\n",
        "                                     data=test_dataset)\n",
        "# Evaluate ensemble model(s) predictions\n",
        "ensemble_results = evaluate_preds(y_true=y_test,\n",
        "                                  y_pred=np.median(ensemble_preds, axis=0)) # take the median across all ensemble predictions\n",
        "ensemble_results"
      ],
      "metadata": {
        "id": "h__huFqXczN_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}